The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.
Obs.	X1	X2	X3	Y
1 		0  	3 	0 	Red
2 		2 	0 	0 	Red
3 		0 	1 	3 	Red
4 		0 	1 	2   Green
5 		−1  0   1   Green
6 		1 	1 	1 	Red
Suppose we wish to use this data set to make a prediction for Y when
X1 = X2 = X3 = 0 using K-nearest neighbors.
(a) Compute the Euclidean distance between each observation and
the test point, X1 = X2 = X3 = 0.
(b) What is our prediction with K = 1? Why?
(c) What is our prediction with K = 3? Why?
(d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or
small? Why?

a) 
Obs.	Dist
1 		3
2 		2
3 		\sqrt(10)
4		\sqrt(5)
5 		\sqrt(2)
6 		\sqrt(3)

b) Ближайший сосед это 5, у него цвет Green, значит ответ - Green
c) Три ближайних соседа это 2, 5, 6 с цветами red, green, red соотвественно, значит нужно присвоить цвет red
d) Так как у нас сложная нелинейная зависимость, границы между классами очень неровные, если мы берем больше K, то получается, что мы сглаживаем границы, а маленькое K дает наибольшую гибкость границ, так что оптимальным будет маленькое K
	